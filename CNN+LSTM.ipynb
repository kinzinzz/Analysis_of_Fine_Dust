{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10LcdLB33Qviz84x9S2Cu5T0rIv-LVZuv",
      "authorship_tag": "ABX9TyOWZYIqMoAYcR4wIv8cE0Db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinzinzz/Analysis_of_Fine_Dust/blob/master/CNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY21QqziQm16",
        "outputId": "8124a75f-eb11-4e7e-af70-4d74de0714f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj8rSuyMY3cn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./drive/MyDrive/2022.11.02.CSV', encoding='cp949')"
      ],
      "metadata": {
        "id": "G3CoxXJTQ1ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['timeStamp'] = pd.to_datetime(df['timeStamp'], format='%Y-%m-%d %H:%M:%S', errors='raise')"
      ],
      "metadata": {
        "id": "tnGI2andVZtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index(\"timeStamp\", inplace=True)"
      ],
      "metadata": {
        "id": "7MWfBuCTV3jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 5\n",
        "batch = 8\n",
        "# train:val:test = 8:1:1\n",
        "\n",
        "train_size = int(len(df)*0.8)\n",
        "\n",
        "train_set = df[0:train_size]\n",
        "val_set = df[train_size:train_size + int(len(df)*0.1)]\n",
        "test_set = df[train_size + int(len(df)*0.1):]"
      ],
      "metadata": {
        "id": "29LfGDJQX2im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 스케일링\n",
        "# input scale\n",
        "scaler_x = MinMaxScaler()\n",
        "\n",
        "scaler_x.fit(train_set.iloc[:,:-1])\n",
        "train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
        "val_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:,:-1])\n",
        "test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:,:-1])\n",
        "\n",
        "# output scale\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "scaler_y.fit(train_set.iloc[:, [-1]])\n",
        "train_set.iloc[:, -1] = scaler_y.transform(train_set.iloc[:, [-1]])\n",
        "val_set.iloc[:, -1] = scaler_y.transform(val_set.iloc[:, [-1]])\n",
        "test_set.iloc[:, -1] = scaler_y.transform(test_set.iloc[:, [-1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2_vUfucXhmP",
        "outputId": "b6456187-a131-4ed5-f5e6-5ef7d41d004a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5e3338e29735>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_set.iloc[:, :-1] = scaler_x.transform(train_set.iloc[:, :-1])\n",
            "<ipython-input-7-5e3338e29735>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:,:-1])\n",
            "<ipython-input-7-5e3338e29735>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_set.iloc[:, :-1] = scaler_x.transform(test_set.iloc[:,:-1])\n",
            "<ipython-input-7-5e3338e29735>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_set.iloc[:, -1] = scaler_y.transform(train_set.iloc[:, [-1]])\n",
            "<ipython-input-7-5e3338e29735>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_set.iloc[:, -1] = scaler_y.transform(val_set.iloc[:, [-1]])\n",
            "<ipython-input-7-5e3338e29735>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_set.iloc[:, -1] = scaler_y.transform(test_set.iloc[:, [-1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def build_dataset(time_series, seq_length):\n",
        "    data_x = []\n",
        "    data_y = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i+seq_length, :]\n",
        "        _y = time_series[i+seq_length, [-1]]\n",
        "\n",
        "        data_x.append(_x)\n",
        "        data_y.append(_y)\n",
        "\n",
        "    return np.array(data_x), np.array(data_y)\n",
        "\n",
        "trainX , trainY = build_dataset(np.array(train_set), seq_length)\n",
        "valX, valY = build_dataset(np.array(val_set), seq_length)\n",
        "testX, testY = build_dataset(np.array(test_set), seq_length)"
      ],
      "metadata": {
        "id": "OSHiuztUX0ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX_tensor = torch.FloatTensor(trainX)\n",
        "trainY_tensor = torch.FloatTensor(trainY)\n",
        "\n",
        "valX_tensor = torch.FloatTensor(valX)\n",
        "valY_tensor = torch.FloatTensor(valY)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX)\n",
        "testY_tensor = torch.FloatTensor(testY)\n",
        "\n",
        "train_dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
        "val_dataset = TensorDataset(valX_tensor, valY_tensor)\n",
        "test_dataset = TensorDataset(testX_tensor, testY_tensor)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, \n",
        "                        batch_size = batch,\n",
        "                        shuffle=True,\n",
        "                        drop_last=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, \n",
        "                        batch_size = batch,\n",
        "                        shuffle=True,\n",
        "                        drop_last=True) "
      ],
      "metadata": {
        "id": "-sq-76XCp18I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델링\n",
        "class FineDustPredictor(nn.Module):\n",
        "    def __init__(self, n_features, n_hidden, seq_length, n_layers):\n",
        "        super(FineDustPredictor, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.seq_length = seq_length\n",
        "        self.n_layers = n_layers\n",
        "        self.n_features = n_features\n",
        "        self.c1 = nn.Conv1d(\n",
        "            in_channels = seq_length,\n",
        "            out_channels = seq_length,\n",
        "            kernel_size = 2,\n",
        "            stride = 1            \n",
        "        )\n",
        "        self.c2 = nn.Conv1d(\n",
        "            in_channels = seq_length,\n",
        "            out_channels = seq_length,\n",
        "            kernel_size = 2,\n",
        "            stride = 1,\n",
        "            dilation = 2\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=n_features,\n",
        "            hidden_size=n_hidden,\n",
        "            num_layers=n_layers,\n",
        "            # dropout=0.3,            \n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=n_hidden, out_features=1, bias = True)\n",
        "    \n",
        "    def reset_hidden_state(self):\n",
        "        self.hidden = (\n",
        "            torch.zeros(self.n_layers, self.seq_length, self.n_hidden),\n",
        "            torch.zeros(self.n_layers, self.seq_length, self.n_hidden)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.c1(x)\n",
        "        x = self.c2(x)\n",
        "        x, self.hidden = self.lstm(x.view(-1, self.seq_length, self.n_features), self.n_hidden)\n",
        "        x = x.view(self.n_hidden, -1)[-1]\n",
        "        y_pred = self.fc(x)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "JgxwY588r1ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_df, val_df, learning_rate=None, num_epochs=100, verbose = 10, patience = 10):\n",
        "    criterion = torch.nn.L1Loss() #loss_fn \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
        "    train_hist = []\n",
        "    val_hist = []\n",
        "    for t in range(num_epochs):\n",
        "\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, samples in enumerate(train_df):\n",
        "            print(batch_idx)\n",
        "            x_train, y_train = samples\n",
        "            \n",
        "            # seq hidden state reset\n",
        "            model.reset_hidden_state()\n",
        "            \n",
        "            y_pred = model(x_train)\n",
        "            \n",
        "            # train loss\n",
        "            loss = criterion(y_pred, y_train) \n",
        "            \n",
        "            # update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_hist.append(epoch_loss / len(train_df))\n",
        "\n",
        "        if val_df is not None:\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                val_loss = 0\n",
        "\n",
        "                for val_batch_idx, val_samples in enumerate(val_df):\n",
        "                    \n",
        "                    x_val, y_val = samples\n",
        "                    model.reset_hidden_state()\n",
        "                    \n",
        "                    val_pred = model(x_val)\n",
        "                    val_step_loss = criterion(val_pred, y_val)\n",
        "                   \n",
        "                    val_loss += val_step_loss\n",
        "                    \n",
        "            val_hist.append(val_loss / len(val_df))\n",
        "\n",
        "            \n",
        "            if t % verbose == 0:\n",
        "                print(f'Epoch {t} train loss: {epoch_loss / len(train_df)} val loss: {val_loss / len(val_df)}')\n",
        "\n",
        "            \n",
        "            if (t % patience == 0) & (t != 0):\n",
        "                \n",
        "                \n",
        "                if val_hist[t - patience] < val_hist[t] :\n",
        "\n",
        "                    print('\\n Early Stopping')\n",
        "\n",
        "                    break\n",
        "\n",
        "        elif t % verbose == 0:\n",
        "            print(f'Epoch {t} train loss: {epoch_loss / len(train_df)}')\n",
        "\n",
        "            \n",
        "    return model, train_hist, val_hist"
      ],
      "metadata": {
        "id": "Xmdki1KSr1g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FineDustPredictor(\n",
        "    n_features=9,\n",
        "    n_hidden=10,\n",
        "    seq_length=seq_length,\n",
        "    n_layers=1\n",
        ")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6BRb9d9njw4",
        "outputId": "4f0ede59-c4e3-4327-af99-899306077f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineDustPredictor(\n",
            "  (c1): Conv1d(5, 5, kernel_size=(2,), stride=(1,))\n",
            "  (c2): Conv1d(5, 5, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
            "  (lstm): LSTM(9, 10)\n",
            "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_hist, val_hist = train_model(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    learning_rate=0.001,    \n",
        "    num_epochs=100,\n",
        "    verbose=10,\n",
        "    patience=50\n",
        ")"
      ],
      "metadata": {
        "id": "XlQRYYNar1nZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "d9a314fb-c83a-4e11-f0ff-aa67eea95a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-01efd413eeaa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, train_hist, val_hist = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-94c7b15034ee>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_df, val_df, learning_rate, num_epochs, verbose, patience)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-412776e05131>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If not PackedSequence input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m                         msg = (\"For batched 3-D input, hx and cx should \"\n\u001b[1;32m    797\u001b[0m                                f\"also be 3-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_hist, label=\"Training loss\")\n",
        "plt.plot(val_hist, label=\"Val loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "X4VP_rrMrI_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 테스트\n",
        "with torch.no_grad(): \n",
        "    pred = []\n",
        "    for pr in range(len(testX_tensor)):\n",
        "\n",
        "        model.reset_hidden_state()\n",
        "\n",
        "        predicted = model(torch.unsqueeze(testX_tensor[pr], 0))\n",
        "        predicted = torch.flatten(predicted).item()\n",
        "        pred.append(predicted)\n",
        "    \n",
        "    pred_inverse = scaler_y.inverse_transform(np.array(pred).reshape(-1, 1))\n",
        "    testY_inverse = scaler_y.inverse_transform(testY_tensor)\n",
        "\n",
        "fig = plt.figure(figsize=(8,3))\n",
        "plt.plot(np.arange(len(pred_inverse)), pred_inverse, label = 'pred')\n",
        "plt.plot(np.arange(len(testY_inverse)), testY_inverse, label = 'true')\n",
        "plt.title(\"Loss plot\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BA6IEihbrVHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}